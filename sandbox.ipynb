{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import json\n",
    "import csv\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'data/math_query.csv'\n",
    "\n",
    "class WikidataConverter():\n",
    "    \"\"\"cleans a csv file of wikidata nodes and finds \n",
    "       english wikipedia articles, if available.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.total_written = 0\n",
    "        self.total_skipped = 0\n",
    "    \n",
    "    def parse(self, input_file, output_file): \n",
    "        \n",
    "        currently_written = 0\n",
    "       \n",
    "        try:\n",
    "            with open(str(output_file), 'r') as f:\n",
    "                reader = csv.reader(f, delimiter=',')\n",
    "                next(reader, None)\n",
    "                existing_ids = {i[0]: i[1] for i in reader}\n",
    "        except:\n",
    "            with open(str(output_file), 'w') as f:\n",
    "                writer = csv.writer(f, delimiter=',')\n",
    "                writer.writerow(['wikidata_id', 'title', 'wikipedia_url', 'wikidata_url'])\n",
    "                existing_ids = {}\n",
    "        \n",
    "        self.total_written = len(existing_ids)\n",
    "        \n",
    "        count = 0   \n",
    "        with open(str(input_file), 'r') as f:\n",
    "            reader = csv.reader(f, delimiter=',')\n",
    "            count = sum(1 for row in f)\n",
    "            \n",
    "        with open(str(input_file), 'r') as infile, open(str(output_file), 'a') as outfile:\n",
    "            wikidata_base = 'http://www.wikidata.org/entity/'\n",
    "            wikipedia_base = 'https://en.wikipedia.org/wiki/'\n",
    "            base_len = len(wikidata_base)\n",
    "            reader = csv.reader(infile, delimiter=',')\n",
    "            writer = csv.writer(outfile, delimiter=',')\n",
    "            next(reader, None) \n",
    "            i = 0\n",
    "            while i < count:\n",
    "                row = next(reader)\n",
    "                data_url = row[0]\n",
    "                data_id = data_url[base_len:]\n",
    "                if data_id in existing_ids:\n",
    "                    continue\n",
    "                else:\n",
    "                    title = row[1]\n",
    "                    existing_ids[data_id] = title\n",
    "                    title = title.replace(' ', '_')\n",
    "                    wikipedia_url = wikipedia_base + title\n",
    "                    try:\n",
    "                        html = urllib.request.urlopen(wikipedia_url).read()\n",
    "                        writer.writerow([data_id, title, wikipedia_url, data_url])\n",
    "                        self.total_written += 1\n",
    "                        currently_written += 1\n",
    "                        percent = round(i/count, 4)\n",
    "                        sys.stdout.write('\\r'+ 'percent_complete: ' + str(percent) + ' currently_written: ' + str(currently_written) + ' total written: ' \\\n",
    "                                         + str(self.total_written))\n",
    "                    except:\n",
    "                        self.total_skipped +=1\n",
    "                        percent = round(i/count, 4)\n",
    "                        sys.stdout.write('\\r'+ 'percent_complete: ' + str(percent) + ' currently_written: ' + str(currently_written) + ' total written: ' \\\n",
    "                                         + str(self.total_written))\n",
    "                i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse = WikidataConverter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percent_complete: 0.0004 currently_written: 55 total written: 55"
     ]
    }
   ],
   "source": [
    "parse.parse('data/academics/query_academics.csv', 'data/academics/academics_en_links.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse.total_skipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.52'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(round(0.522, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from selenium.webdriver import Firefox\n",
    "from bs4 import BeautifulSoup as soup\n",
    "\n",
    "def get_expand_buttons(browser):\n",
    "    \"\"\"Return a list of expand buttons to click on.\"\"\"\n",
    "    return browser.find_elements_by_xpath(\"//span[@title='expand']\")\n",
    "\n",
    "total_links = []\n",
    "\n",
    "def expand_all_categories(browser):\n",
    "    \"\"\"Expand all categories on the page.\"\"\"\n",
    "    expand_buttons = get_expand_buttons(browser)\n",
    "    global total_links\n",
    "    total_links = []\n",
    "    i = 0\n",
    "    while i<6:\n",
    "        for button in expand_buttons:\n",
    "            if button.is_displayed():\n",
    "                button.click()\n",
    "                time.sleep(0)\n",
    "        lst = []\n",
    "        for a in browser.find_elements_by_xpath('.//a'):\n",
    "            lst.append(a.get_attribute('href'))\n",
    "        total_links.append(lst)\n",
    "        expand_buttons = get_expand_buttons(browser)\n",
    "        \n",
    "        i += 1\n",
    "        \n",
    "def main():\n",
    "    \"\"\"Open the page and expand all categories.\"\"\"\n",
    "    browser = Firefox()\n",
    "    browser.get('https://en.wikipedia.org/wiki/Special:CategoryTree?target=mathematics&mode=all&namespaces=&title=Special%3ACategoryTree')\n",
    "    expand_all_categories(browser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser = Firefox()\n",
    "browser.get('https://en.wikipedia.org/wiki/Special:CategoryTree?target=mathematics&mode=all&namespaces=&title=Special%3ACategoryTree')\n",
    "expand_all_categories(browser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for links in total_links:\n",
    "    print(len(links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = []\n",
    "for a in browser.find_elements_by_xpath('.//a'):\n",
    "    lst.append(a.get_attribute('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
