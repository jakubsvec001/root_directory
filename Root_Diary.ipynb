{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Root Diary**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **The Question**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project started with a simple question: how many pages on wikipedia are related to mathematics? \n",
    "\n",
    "As it turns out, the answer is not so simple.\n",
    "\n",
    "After searching with Mathematics category page, I found this line:\n",
    "\n",
    "But after reaching out to the category page editors, I was told that the number was not updated since 2015, and that other numbers were floating around from around the same time that were estimating the number to be around 17,000 and 21,000. The editors quicly deleted the line.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **The Problem**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wikipedia's system of categorizing content is all over the place. \n",
    "\n",
    "#### _Wikipedia Methods of Categorizing:_\n",
    "\n",
    "- [Portal page](https://en.wikipedia.org/wiki/Portal:Contents/Portals#Mathematics_and_logic)\n",
    "- [Contents page](https://en.wikipedia.org/wiki/Portal:Contents/Mathematics_and_logic)\n",
    "- [Category page](https://en.wikipedia.org/wiki/Category:Mathematics)\n",
    "- [Outline page](https://en.wikipedia.org/wiki/Outline_of_mathematics)\n",
    "- [Areas of Mathematics page](https://en.wikipedia.org/wiki/Areas_of_mathematics)\n",
    "- [Indices](https://en.wikipedia.org/wiki/Category:Mathematics-related_lists)\n",
    "- [Overviews](https://en.wikipedia.org/wiki/Category:Mathematics-related_lists)\n",
    "- [Glossaries](https://en.wikipedia.org/wiki/Category:Mathematics-related_lists)\n",
    "\n",
    "Of these, Mathematics is organized using subset of these structures:\n",
    "\n",
    "- [Mathematics Portal page](https://en.wikipedia.org/wiki/Portal:Contents/Portals#Mathematics_and_logic)\n",
    "- [Mathematics Contents page](https://en.wikipedia.org/wiki/Portal:Contents/Mathematics_and_logic)\n",
    "- [Mathematics Category page](https://en.wikipedia.org/wiki/Category:Mathematics)\n",
    "- [Mathematics Outline page](https://en.wikipedia.org/wiki/Outline_of_mathematics)\n",
    "- [Areas of Mathematics page](https://en.wikipedia.org/wiki/Areas_of_mathematics)\n",
    "- [Mathematics Lists](https://en.wikipedia.org/wiki/Category:Mathematics-related_lists)\n",
    "- [lists of lists of lists](https://en.wikipedia.org/wiki/List_of_lists_of_lists#Mathematics_and_logic)\n",
    "\n",
    "This also includes the [list of mathematics topics page](https://en.wikipedia.org/wiki/Lists_of_mathematics_topics)\n",
    "\n",
    "\n",
    "\n",
    "Contents Overview Outlines Lists Portals Glossaries Categories Indices\n",
    "\n",
    "\n",
    "#### _Wikidata Methods of Categorizing:_\n",
    "\n",
    "Outside of the Wikipedia project, the WikiMedia Foundation also includes the the Wikidata project. This project is based on a graph database (using the Sparql language) that is community edited organize data based on their interconnections.\n",
    "\n",
    "```sparql\n",
    " SELECT distinct ?item ?article ?sitelink ?linkTo WHERE {\n",
    "       { ?item wdt:P361* wd:Q395 .}\n",
    "       union\n",
    "       { ?item wdt:P361/wdt:P279* wd:Q395 .}\n",
    "       union\n",
    "       { ?item wdt:P31/wdt:P279* wd:Q1936384 .}\n",
    "       union\n",
    "       { ?item wdt:P921/wdt:P279* wd:Q395 .}\n",
    "       optional \n",
    "       { ?sitelink ^schema:name ?article .\n",
    "         ?article schema:about ?item ;\n",
    "         schema:isPartOf <https://en.wikipedia.org/> .\n",
    "       }\n",
    "       OPTIONAL { ?item wdt:P361 ?linkTo. }\n",
    "       SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\". }\n",
    "       }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While graph databases are dope, when humans are used to create nodes and relationships the data is not entirely reliable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data Wrangling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Wikidata Search_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the results from the Wikidata database search did not answer our original question, we can use the results to create a seed dataset.\n",
    "\n",
    "While ~800 articles may be enough to train a Natural Language Processing model, more data is always better.\n",
    "\n",
    "Even though the Wikidata query is complex and not likely something that an average wikimedia user is capable of writing, we will use this number as our evaluation metric. This metric represents the ability of wikimedia contributors to structure content around a major category.\n",
    "\n",
    "### _Webscraping_\n",
    "\n",
    "To accomplish this, we can turn to scraping Wikipedia category strucutures. For each category structure page, we can find all the internal wiki links, store them in a container. This represents a depth search of one. Next, scraping the pages in the container for more internal wiki links results in a depth search of two. The number of iterations we perform on the articles in our container will determine the depth of our search. This can result in a large set of links, however it is a bit crude since links can be arbitrarily added to a page.\n",
    "\n",
    "While web scraping is fun, the Wikimedia Foundation has already developed a program for us that does this for us. It is called the [PetScan](https://petscan.wmflabs.org/).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _PetScan searching_\n",
    "\n",
    "Contributors to the Wikimedia Foundation have built a tool for finding articles based on categories. It is a powerful tool, but it is too crude to answer our original question. Instead, we will use it to find our seed dataset. \n",
    "\n",
    "PetScan takes in a category parameter and a depth of search parameter. This roughly translates to the same process that we would have used if we were scarping Wikipedia ourselves.  \n",
    "\n",
    "Searching the Mathematics category with a depth of one yields 1,216 results. Kinda disappointing. A depth search of two yields 11,393 results, and a depth search of three yiedls 34,013. Well, that escalates quickly. However, with larger depths, we are also getting more noise in the results. We'll pick a depth search of one, and supplement this set by also searching mathematics subtopics using Petscan. After searching the Algebra, Arithmetic, Calculus, Combinatorics, Game Theory, Geometry, Linear Algebra, Probability, Statistics, and Topology categories with a depth of one, respectively, we have a total of about 10,000 results to add to our Wikidata results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Putting it All Together_\n",
    "\n",
    "After combining all of the articles we gathered, we can delete any duplicates and make a count of our bounty.\n",
    "\n",
    "_Seed Dataset Count: 10,338_\n",
    "\n",
    "If that was all we had to do, we'd be golden. But we want to be thorough and check that our seed dataset is purged of bad articles.\n",
    "\n",
    "### _Manual Cleaning_\n",
    "\n",
    "There is no way around it: each item in our seed dataset needs to be checked and purged if it is not related to mathematics, or is only tangentially related to mathematics. Common articles that were purged included people related to mathematics, articles about academic journals, and universities.\n",
    "\n",
    "_Seed Dataset Count after Cleaning: 8,687_\n",
    "\n",
    "_Hours of Manual Cleaning: 4.25_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noise Dataset\n",
    "\n",
    "In order to train a model to teach it how to recognize mathematics articles, we need a dataset of non-mathematics articles. For this we will use Physics, Biology, Chemistry, and Computer Science articles. We will again use Petscan with a depth of one search.\n",
    "\n",
    "Physics:\n",
    "\n",
    "Biology:\n",
    "\n",
    "Chemistry:\n",
    "\n",
    "Computer Science:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Evaluation Metric**\n",
    "\n",
    "As described above, the evaluation metric we will be using is 800 mathematics related articles. This metric was derived from a search of the Wikidata graph database for mathematics-related nodes with associated English wikipedia articles. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Lets See What We Can Do with Our Seed Dataset!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traversing Wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "['Wikipedia Data Science: Working with the Worldâ€™s Largest Encyclopedia'](https://towardsdatascience.com/wikipedia-data-science-working-with-the-worlds-largest-encyclopedia-c08efbac5f5c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traversing XML tree iteratively:\n",
    "xml.sax\n",
    "\n",
    "parsing wikipedia content pages:\n",
    "mwparserfromhell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Next Steps**\n",
    "\n",
    "During this project, I played around with the gensim python package for creating corpuses from Wikipedia data. This package handles many if not most of the tasks we performed using xml.sax, mwparserfromhell and sklearn. Not only is a powerful tool for creating a natural language proecessing pipeline, but it is also otimized for speed with built-in multiprocessing options."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Topic Searching\n",
    "[Concept Search Project](http://mccormickml.com/2017/02/22/concept-search-on-wikipedia/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
